{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os, cv2, numpy as np, pandas as pd, torch, torch.nn as nn\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\nfrom PIL import Image\nfrom tqdm import tqdm\n\nBASE1 = \"/kaggle/input/soil-classification-2/soil_competition-2025/\"\nBASE2 = \"/kaggle/input/soil-classification/soil_classification-2025/\"\nTR_IMG1 = os.path.join(BASE1, \"train\")\nTR_IMG2 = os.path.join(BASE2, \"train\")\nTE_IMG  = os.path.join(BASE1, \"test\")\nLBL_CSV1= os.path.join(BASE1, \"train_labels.csv\")\nLBL_CSV2= os.path.join(BASE2, \"train_labels.csv\")\nIDS_CSV = os.path.join(BASE1, \"test_ids.csv\")\n\ndef smart_crop(img, thr=15):\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    _, mask = cv2.threshold(gray, thr, 255, cv2.THRESH_BINARY)\n    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if not cnts: return img\n    x, y, w, h = cv2.boundingRect(max(cnts, key=cv2.contourArea))\n    return img[y:y+h, x:x+w]\n\ndf1 = pd.read_csv(LBL_CSV1).drop_duplicates(\"image_id\")\ndf2 = pd.read_csv(LBL_CSV2).drop_duplicates(\"image_id\")\n\ndf2[\"label\"] = 1  # treat all multiâ€‘class soil as soil\n\ndf1[\"path\"] = df1[\"image_id\"].apply(lambda x: os.path.join(TR_IMG1, x))\ndf2[\"path\"] = df2[\"image_id\"].apply(lambda x: os.path.join(TR_IMG2, x))\n\ndf = pd.concat([df1, df2], ignore_index=True)\n\nif df[\"label\"].nunique()==2:\n    cw = compute_class_weight(\"balanced\", classes=np.array([0,1]), y=df.label)\n    class_wt = torch.tensor(cw, dtype=torch.float32)\nelse:\n    class_wt = None\n\nclass SoilDataset(Dataset):\n    def __init__(self, frame, tfm):\n        self.df = frame.reset_index(drop=True)\n        self.tfm = tfm\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = cv2.imread(row.path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = smart_crop(img)\n        try:\n            img = Image.fromarray(img)\n            img = self.tfm(img)\n        except Exception:\n            fallback = Image.fromarray(np.zeros((256,256,3), dtype=np.uint8), mode=\"RGB\")\n            img = self.tfm(fallback)\n        label = int(row.label) if \"label\" in row else -1\n        return img, label, os.path.basename(row.path)\n\ntrain_tf = T.Compose([\n    T.Resize((256,256)),\n    T.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(0.9,1.1)),\n    T.RandomHorizontalFlip(),\n    T.RandomVerticalFlip(),\n    T.ColorJitter(0.3,0.3,0.2,0.05),\n    T.ToTensor(),\n    T.RandomErasing(p=0.2, scale=(0.01,0.05), value=0),\n    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nval_tf = T.Compose([\n    T.Resize((256,256)),\n    T.ToTensor(),\n    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_one_model(rnd):\n    print(f\"\\nðŸ” Boost Round {rnd+1}\")\n    tr_df, val_df = train_test_split(df, test_size=0.2, stratify=df.label, random_state=42+rnd)\n    tr_ld = DataLoader(\n        SoilDataset(tr_df, train_tf),\n        batch_size=32, shuffle=True, num_workers=2\n    )\n    vl_ld = DataLoader(\n        SoilDataset(val_df, val_tf),\n        batch_size=32, shuffle=False, num_workers=2\n    )\n\n    model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n    model = model.to(DEVICE)\n\n    loss_fn = nn.CrossEntropyLoss(weight=class_wt.to(DEVICE) if class_wt is not None else None)\n    opt = torch.optim.Adam(model.parameters(), 1e-4)\n    scaler = torch.amp.GradScaler(enabled=DEVICE.type==\"cuda\")\n\n    best = 0\n    for ep in range(15):\n        model.train(); tot=0\n        for x,y,_ in tqdm(tr_ld, leave=False):\n            x,y = x.to(DEVICE), y.to(DEVICE)\n            opt.zero_grad()\n            with torch.amp.autocast(device_type=DEVICE.type):\n                loss = loss_fn(model(x), y)\n            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n            tot += loss.item()\n        # validation\n        model.eval(); pr, gt = [],[]\n        with torch.no_grad():\n            for x,y,_ in vl_ld:\n                out = model(x.to(DEVICE))\n                pr += out.argmax(1).cpu().tolist(); gt += y.tolist()\n        f1 = f1_score(gt, pr)\n        print(f\"Epoch {ep+1:02d} | loss {tot/len(tr_ld):.4f} | F1 {f1:.4f}\")\n        if f1>best:\n            best=f1; torch.save(model.state_dict(), f\"/kaggle/working/best_{rnd}.pth\")\n    return model\n\n# ------------ Ensemble Train ----------\nmodels=[]\nfor r in range(3):\n    m=train_one_model(r)\n    m.load_state_dict(torch.load(f\"/kaggle/working/best_{r}.pth\")); m.eval(); models.append(m)\n\n# -------------- Inference -------------\ntest_df = pd.read_csv(IDS_CSV)\ntest_df[\"path\"] = test_df.image_id.apply(lambda x: os.path.join(TE_IMG,x))\n\ntest_ld = DataLoader(\n    SoilDataset(test_df, val_tf),\n    batch_size=32, shuffle=False, num_workers=2\n)\npr_all, ids = [], []\nwith torch.no_grad():\n    for x,_,nm in tqdm(test_ld):\n        x=x.to(DEVICE)\n        probs = sum(torch.softmax(m(x),1) for m in models)/len(models)\n        pr_all += probs.argmax(1).cpu().tolist(); ids += nm\n\nsub = pd.DataFrame({\"image_id\":ids,\"label\":pr_all})\nsub = test_df.set_index(\"image_id\").join(sub.set_index(\"image_id\")).reset_index()\nsub.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"âœ… submission.csv saved!\")\ndisplay(sub.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}